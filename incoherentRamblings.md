# Incoherent Ramblings/Project log
---
So, I realized that putting all of my thoughts into the README is probably not the greatest idea. The purpose of this file is to journal my thought process through each task and log my progress.

## Before 10-30-2024
1. The transcripts taken from the streams are not punctuated, are formatted weirdly, and contain the occasional typo/incorrect word. This may interfere with training and result in the model not behaving as desired. A fix for this could be to use another LLM to fix the sentences, but a problem with this approach is the inconsistency between different chats. This problem may also play together with the problem of limited prompt sizes as if the transcripts are chopped up and fed into the LLM in different pieces, the resulting transcript may include inconsistencies from different runs. Might need to learn how to manipulate prompts so the outputs are more consistent. Playing with the prompts appears to make the outputs more consistent, although not completely consistent. Another solution could be to use another tool to pull the transcripts from the streams themselves, which could yield higher quality transcripts with better context. Using OpenAI Whisper could result in higher quality transcripts, although generating the transcripts will take more time. This should be tested.

2. I don't even know where to begin on getting the prompt/response pairs for fine tuning, especially with the inconsistencies in the model output as stated in the previous problem. The ideas for merging chat into one prompt along side with using the fact that the streamer often reads a message before responding to it rely on getting an LLM to do it if I don't want to do it manually. Also I really don't want to beg clippers and even if I did, I don't know if I would get enough data from them.

3. I am now trying to use OpenAI Whisper to generate the transcripts as the quality is a lot higher than the youtube vtt one. This method comes with its own problems though. Since the API only supports files that are less than 25MB, it is necessary to cut the audio into chunks and make transcripts on every chunk. And of course, as I was writing that last sentence, the script I ran to generate a transcript from the 260MB audio file of an entire stream finished running and generated everything, so I don't know if I need to split these files anymore. More testing is needed. Additionally, the "awawawa's" are cut and I think they are important for training, so if possible, I need to get Whisper to recognize those. I think I can do this with prompting, which Whisper should support. To format the data to be able to be used for fine-tuning, I'm also going to need timestamps, so I also need a way for Whisper to give me those as well. This should be supported, but the API is a bit weird as the whisper github page and what's on the OpenAI API kind of contradict each other, so it could be a headache to get working. WHY DO THESE STREAMS HAVE DIFFERENT FILETYPES? Chopping the audio seems to produce better results. However, both methods do produce hallucinations. The transcribe function has a parameter called hallucination_silence_threshold, but I'm not sure how long to make it to be effective. It does not seem like I can get the timestamps with Whisper. This makes it so that Whisper is unable to generate prompt/response data for fine-tuning, but it still can be used for training. Wait, there has got to be alternatives for Whisper

4. Learn NLP. What if I just go through the audio and cut it into sections when the topic shifts? There's got to be a way to do this right? How hard could it be to use sentiment analysis to determine when the topic of the conversation changes? Maybe I could get the transcripts with Whisper, use sentiment analysis to determine topic shifts, and then map each segment to a timestamp in the stream.

## 10-30-2024
It turns out that the idea I had yesterday (number 4 in previous section) is called "Text Segmentation". Furthermore, from skimming this [article](https://www.naveedafzal.com/posts/an-introduction-to-unsupervised-topic-segmentation-with-implementation/), the approach is similar to what I had in mind. However, the approach described in this article only uses lexical info to define boundaries and can't pick up on other cues in human conversation that defines a topic shift. This might cause some imperfections in the sectioning, but it shouldn't matter for now as we would be using the text data generated by Whisper, so those cues are gone anyways. There are still some hallucinations during segments with only background music. Maybe I need to do some preprocessing on the original audio to filter out the music.