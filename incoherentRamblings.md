# Incoherent Ramblings/Project log
---
So, I realized that putting all of my thoughts into the README is probably not the greatest idea. The purpose of this file is to journal my thought process through each task and log my progress.

## Before 10-30-2024
1. The transcripts taken from the streams are not punctuated, are formatted weirdly, and contain the occasional typo/incorrect word. This may interfere with training and result in the model not behaving as desired. A fix for this could be to use another LLM to fix the sentences, but a problem with this approach is the inconsistency between different chats. This problem may also play together with the problem of limited prompt sizes as if the transcripts are chopped up and fed into the LLM in different pieces, the resulting transcript may include inconsistencies from different runs. Might need to learn how to manipulate prompts so the outputs are more consistent. Playing with the prompts appears to make the outputs more consistent, although not completely consistent. Another solution could be to use another tool to pull the transcripts from the streams themselves, which could yield higher quality transcripts with better context. Using OpenAI Whisper could result in higher quality transcripts, although generating the transcripts will take more time. This should be tested.

2. I don't even know where to begin on getting the prompt/response pairs for fine tuning, especially with the inconsistencies in the model output as stated in the previous problem. The ideas for merging chat into one prompt along side with using the fact that the streamer often reads a message before responding to it rely on getting an LLM to do it if I don't want to do it manually. Also I really don't want to beg clippers and even if I did, I don't know if I would get enough data from them.

3. I am now trying to use OpenAI Whisper to generate the transcripts as the quality is a lot higher than the youtube vtt one. This method comes with its own problems though. Since the API only supports files that are less than 25MB, it is necessary to cut the audio into chunks and make transcripts on every chunk. And of course, as I was writing that last sentence, the script I ran to generate a transcript from the 260MB audio file of an entire stream finished running and generated everything, so I don't know if I need to split these files anymore. More testing is needed. Additionally, the "awawawa's" are cut and I think they are important for training, so if possible, I need to get Whisper to recognize those. I think I can do this with prompting, which Whisper should support. To format the data to be able to be used for fine-tuning, I'm also going to need timestamps, so I also need a way for Whisper to give me those as well. This should be supported, but the API is a bit weird as the whisper github page and what's on the OpenAI API kind of contradict each other, so it could be a headache to get working. WHY DO THESE STREAMS HAVE DIFFERENT FILETYPES? Chopping the audio seems to produce better results. However, both methods do produce hallucinations. The transcribe function has a parameter called hallucination_silence_threshold, but I'm not sure how long to make it to be effective. It does not seem like I can get the timestamps with Whisper. This makes it so that Whisper is unable to generate prompt/response data for fine-tuning, but it still can be used for training. Wait, there has got to be alternatives for Whisper

4. Learn NLP. What if I just go through the audio and cut it into sections when the topic shifts? There's got to be a way to do this right? How hard could it be to use sentiment analysis to determine when the topic of the conversation changes? Maybe I could get the transcripts with Whisper, use sentiment analysis to determine topic shifts, and then map each segment to a timestamp in the stream.

## 10-30-2024
It turns out that the idea I had yesterday (number 4 in previous section) is called "Text Segmentation". Furthermore, from skimming this [article](https://www.naveedafzal.com/posts/an-introduction-to-unsupervised-topic-segmentation-with-implementation/), the approach is similar to what I had in mind. However, the approach described in this article only uses lexical info to define boundaries and can't pick up on other cues in human conversation that defines a topic shift. This might cause some imperfections in the sectioning, but it shouldn't matter for now as we would be using the text data generated by Whisper, so those cues are gone anyways. There are still some hallucinations during segments with only background music. Maybe I need to do some preprocessing on the original audio to filter out the music. This is quite complex, I will try again tomorrow when my mind is clearer.

## 10-31-2024
Happy Halloween! I think today's biggest nightmare is everything that lies before me to get the prompt/response pairs working. I did end up getting the topic segmentation working by taking the second LLM based segmentation approach in the [article](https://www.naveedafzal.com/posts/an-introduction-to-unsupervised-topic-segmentation-with-implementation/) I mentioned yesterday. However, the segmentation isn't perfect as sometimes a topic is split up into multiple topics or small segments are appended to other ones. I don't know what I would do during the segmentation process to make it better, but since the current system does alright, we could do some modifications to the result to improve our output. Now that the data is segmented, it will be easier to throw out some of the data that was Whisper failed to properly transcribe, which will benefit both datasets. The next steps after segmentation is mapping each segment to a timestamp in the VOD and finding/generating a reasonable prompt from chat at that time. To find the timestamp in the video, there are probably some libraries out there that could assist with that. Some starting points to look are pysrt, DeepSpeech, and kaldi. Additionally, to find a reasonable prompt, we could look at each chat message and determine how related each message is to the response. The chat messages that should be considered should either range between the time of the n previous sections and the current section, the last n messages, or the last n seconds. This approach could also be used to finetune the segmented responses as if multiple sequential segments are related enough, they probably should be merged into one segment. Generating a prompt based on chat as a collective could be trickier and at this moment, I'd say the necessity of doing it is 50/50. We also need a way to determine whether chat did not affect a section. This could be done by setting a threshold of relatedness to consider.

TODO (not definite):
1. Find the timestamps corresponding to each section
2. Find a way to match chat messages to sections in the transcripts
3. Finetune the transcripts
4. Generate prompts based on chat hivemind (may not be necessary)
5. Combine each part into one easy to use script
6. Run script on all streams
7. Train model

Somewhere before number 6, I need to categorize the streams.

## 11-04-2024
Alignment seems like it's going to be tricky. Two libraries I have found refer to [this](https://pytorch.org/audio/main/tutorials/forced_alignment_tutorial.html) Pytorch tutorial, so it maybe worth looking into. However, when I tried to use one of these libraries, it wants ~180GB of VRAM. 180GB of VRAM is crazy, the H100 has 80GB and this thing wants more than double that. Looking at the segmented transcript again, it's actually kind of bad, so maybe I should revisit that. I've been at a block since the last entry. I might just need to take a step back and find a way to generate timestamps with the transcript (maybe use a subtitle generation tool) or continue looking for a way to get the timestamps after segmentation (maybe find the timestamps for each word in the transcript and only use the ones for the first word of each segment?). This doesn't handle the bad segmentation either. This is turning into a headache, so I'm going to take a step back for now.

## 11-05-2024
I rewrote the transcription tool to get timestamps of each chunk of words respective to time. The quality of these transcripts should also be better as I am filtering out text with low probability scores, which gets rid of hallucinations. However, if there is a brief pause in the middle of a topic, the topic will be split into multiple parts as each chunk is respective to time. This is kind of like the VTT Parser, but better. I'll have to see how well this new method works with the topic segmentation.

## 11-06-2024
There's a problem with whisper transcription where it will separate one sentence into each word. The timestamps are right next to each other, so I could probably do some processing to recombine them. Segmentation, on the other hand, remains tricky as there doesn't appear to be an easy way to carry the timestamp data into the segmented text. I'll keep on refining whisper transcription for now. Maybe another method of segmentation is needed.

## 11-11-2024
I fixed some of the whisper stuff a few days ago. Now each segment in a transcript is a period of uninterrupted speaking by the streamerq. This might come with some built in segmentation by topic, but many topics are still in a bunch of chunks. I also had the idea of soley using superchats for prompt/response pairs as those are usually answered and I can use more streams for fine tuning. I wrote a script that only parses the superchat/membership messages in each stream and gets the timestamp of those messages. If I could use the timestamp for a supa, scan the transcript for sections around the same time, and find some way to filter out the irrelevant sections, I think that would be it for getting the prompt/response pairs. That last part about filtering out the irrelevant parts is going to be the hard part.

## 11-13-2024
Well I have something now, although it's not really good. I have a thing that takes a chat message and uses its timestamp to find transcript sections around that time. Then, a sentence transformer model is used to calculate the similarity of the chat message to each of the transcript sections and everything below a certain threshold is not included. This method, however, is not very good at getting "good" data as if a section contains something irrelevant to the chat message as well as something that is relevant, the irrelevant part is kept in. Additionally, the streamer often reads the chat message out loud before responding to it, so the response will often contain the chat message again and often only the reading of the chat message. On top of that, sometimes relevent sections will straight up not get picked up. I will need some way to filter out the irrelevant data and repetition, which would probably require moving away from soley using sentence embeddings, but sentence embeddings may still be useful in combination with other methods. I'm not entirely sure what to search/ask an LLM to get what I am looking for, so I need to think about this more.

## 11-20-2024
At this point I'm desperate lol. The text segmentation I had before does a pretty good job already, although it's sometimes off by a sentence or 2 and still sometimes separates what should be 1 chunk into multiple, but it's better than anything else I've tried. I might just get the timestamp from the whole transcript, find the corresponding chunk, find the similarities between the chat message and the surrounding chunks, and just use whatever is similar. That's for me to do some other time though. I haven't pushed to the github repo in a while, so I should probably do that now
    
## 11-22-2024
What if I used collab streams to get prompt/response pairs? If there is a way to get the transcript of those streams and differentiate between who is talking, this could work. After all, collab streams are just multiple people holding a conversation. Additionally, if possible, I think the quality of the data will be better than getting the pairs off supas and if things go right, it might even be easier than getting them off supas. Some considerations for potential problems might include knowing which speaker is the one I want, dealing with 2 people speaking at the same time, and making sure prompts and responses are from the appropiate people. A good starting point for this could be Speaker diarization with speech to text. Also worth watching a few collab streams to get a better idea of how I can best collect the data. Worst case scenario, I do everything by hand. Finding something to do diarization that works is proving to be a bit difficult. I have gotten nowhere today.

## 1-8-2025
Happy New Year! This project is dead as I have realized getting AI to do the chat segmentation is too unreliable and I don't want to do this by hand. Maybe I'll come back to this, maybe not, but this project is on hold until I have an idea that can solve these problems.